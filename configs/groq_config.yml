# put here token generated at https://console.groq.com/keys
api_key: ""

# one of the models supported by groq platform: llama3-70b-8192, mixtral-8x7b-32768, gemma-7b-it
llm_model: "llama3-8b-8192"

# max tokens for prompt generation
max_tokens: 160

# temperature interval for generator, [min_val, max_val]; Should be within [0, 1]
temperature: [0.25, 0.6]

# random seed
seed: -1