# the vllm model that will be used for the prompt generation
# awq - quantization level of the model supported by the vllm
llm_models: ["meta-llama/Meta-Llama-3.1-8B-Instruct"]

# max tokens for prompt generation
max_tokens: 160

# defines the maximum length of the model output
max_model_len: 1024

# temperature interval for generator, [min_val, max_val]; Should be within [0, 1]
temperature: [0.25, 0.5]

# random seed
seed: 0