# one of the models supported by groq platform: llama3-70b-8192, mixtral-8x7b-32768, gemma-7b-it
groq_llm_model: "llama3-8b-8192"

# transformers model that will be used for generating prompt dataset in offline mode
transformers_llm_model: "mistralai/Mistral-7B-Instruct-v0.2"

# the llm model that will be used for checking the quality of the prompts
transformers_llm_model_prompt_checker: "google/gemma-1.1-7b-it"

# the vllm model that will be used for the prompt generation
# awq - quantization level of the model supported by the vllm
vllm_llm_model: "casperhansen/llama-3-8b-instruct-awq"

# put here token generated at https://console.groq.com/keys
groq_api_key: ""

# hugging face api token, can be generated within your account on the platform. Will be required
# for downloading gemma LLM.
hugging_face_api_key: ""

# the prompt that will be used for generating the dataset.
# NOTE: member_placeholder and prompts_num are mandatory placeholders.
# prompts_num will be replaced with prompts_num from this config;
# member_placeholder will be replaced with one of the strings stored in obj_categories list.

prompt: "Generate a prompt dataset for generating 3D models.
         Each prompt should define a single 3D object that can be generated as a 3D mesh.
         Each object should be different and must be strictly picked from the member_placeholder category.
         Each prompts should be on the new line with between three to ten words.
         Generate a numbered list of prompts_num prompts.
        "

# Categories of objects from where the LLM model could sample the data.
obj_categories: ["humanoids", "animals", "monsters", "robots", "buildings", "nature", "vehicles", "weapons and equipments",
                 "food and drinks", "gadgets and electronics", "decorative elements", "furniture", "jewelry"
                ]

# Words that prompts should npt contain. Prompts with these words will be removed from the dataset and filtering stage.
filter_prompts_with_words: ["sky", "skies", "river", "ocean", "sea", "garden", "wind", "field", "terrain", "family", "tow", "city", "accessories",
                            "jungle", "forest", "space", "pool", "pond", "I", "i", "fields", "horizon", "oops", "hillside", "underwater",
                            "floor", "grass", "nature", "mist", "air", "waterfall", "music", "sunset", "sunrise", "beach", "room", "cluster", "accents",
                            "melody", "wind", "winds", "tale", "sure", "prompts", "prompt", "sunbeam", "water", "word", "words", "money", "cave", "copy",
                            "vacuum", "outdoor", "to", "us", "miami", "kidding", "time", "sunken", "point", "like", "breathing", "whoops", "labyrinth",
                            "village", "seaside", "cloud", "clouds", "exterior", "no", "unit", "harbor", "window", "grip", "island", "song", "ambiance",
                            "orbit", "hope", "melody", "animate"
                            ]

# amount of prompts to generate per category.
prompts_num: "fifty"

# specify number of times you want to run the model (total prompt size: prompts_num x len(obj_categories) x iteration_num
iteration_num: 1

# file where to output the prompts (.txt file)
#prompts_output_file: "prompt_dataset.txt"
prompts_output_file: "prompt_dataset.txt"

# parameters for the llama-cpp loader
llm_model:
    # RNG seed, -1 for random [llama-cpp & Groq]
    seed: -1

    # The maximum number of tokens to generate prompts
    max_tokens: 256
